{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import camelot as cam\n",
    "import pdfplumber\n",
    "import json\n",
    "import argparse\n",
    "from typing import List, Dict, Union\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_pdf_pages(pdf_name: str) -> dict[str, list[int]]:\n",
    "    \"\"\"Classify PDF pages as digital or scanned\"\"\"\n",
    "    page_groups = {\"digital\": [], \"scanned\": []}\n",
    "    try:\n",
    "        doc = fitz.open(pdf_name)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            if page.get_text().strip():\n",
    "                page_groups[\"digital\"].append(page_num)\n",
    "            else:\n",
    "                page_groups[\"scanned\"].append(page_num)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to classify pages: {e}\")\n",
    "    return page_groups\n",
    "\n",
    "def create_page_ranges(page_numbers: list[int]) -> list[str]:\n",
    "    \"\"\"Convert list of page numbers to human-readable ranges (1-indexed)\"\"\"\n",
    "    if not page_numbers:\n",
    "        return []\n",
    "    \n",
    "    ranges = []\n",
    "    start = prev = page_numbers[0]\n",
    "    for num in page_numbers[1:]:\n",
    "        if num != prev + 1:\n",
    "            ranges.append(f\"{start+1}-{prev+1}\" if start != prev else str(start+1))\n",
    "            start = num\n",
    "        prev = num\n",
    "    ranges.append(f\"{start+1}-{prev+1}\" if start != prev else str(start+1))\n",
    "    return ranges\n",
    "\n",
    "def locate_table_title(pdf_name: str, keywords: List[str]) -> List[int]:\n",
    "    \"\"\"Find pages with table titles in top 35% of page\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_name)\n",
    "        matching_pages = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            for block in page.get_text(\"blocks\"):\n",
    "                x0, y0, x1, y1, text, _, _ = block\n",
    "                if y0 < page.rect.height * 0.35:\n",
    "                    if any(kw.lower() in text.lower() for kw in keywords):\n",
    "                        matching_pages.append(page_num)\n",
    "                        break\n",
    "        return matching_pages\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Title location failed: {e}\")\n",
    "\n",
    "def extract_table_pages(pdf_name: str, title_page: int) -> List[int]:\n",
    "    \"\"\"Identify pages containing continuous table data\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_name)\n",
    "        table_pages = [title_page]\n",
    "        \n",
    "        # Get reference column count from title page\n",
    "        title_tables = cam.read_pdf(pdf_name, pages=str(title_page+1), flavor=\"lattice\")\n",
    "        if not title_tables:\n",
    "            return table_pages\n",
    "        \n",
    "        ref_columns = len(title_tables[0].df.columns)\n",
    "        current_page = title_page + 1\n",
    "\n",
    "        while current_page < len(doc):\n",
    "            try:\n",
    "                tables = cam.read_pdf(pdf_name, pages=str(current_page+1), flavor=\"lattice\")\n",
    "                if tables and len(tables[0].df.columns) == ref_columns:\n",
    "                    table_pages.append(current_page)\n",
    "                    current_page += 1\n",
    "                else:\n",
    "                    break\n",
    "            except Exception:\n",
    "                break\n",
    "        return table_pages\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Table page extraction failed: {e}\")\n",
    "\n",
    "def get_continued_tables(tables, threshold: int = 15):\n",
    "    \"\"\"Group spatially continuous tables across pages\"\"\"\n",
    "    try:\n",
    "        continued_tables = []\n",
    "        current_group = []\n",
    "        page_height = 842  # Average PDF page height in points\n",
    "\n",
    "        for i, table in enumerate(tables):\n",
    "            if not current_group:\n",
    "                current_group.append(table)\n",
    "                continue\n",
    "\n",
    "            prev_table = tables[i-1]\n",
    "            prev_bottom = prev_table._bbox[1]\n",
    "            curr_top = table._bbox[3]\n",
    "\n",
    "            if (table.page == prev_table.page + 1 and\n",
    "                len(table.cols) == len(prev_table.cols) and\n",
    "                prev_bottom < (threshold/100)*page_height and\n",
    "                curr_top > (1 - threshold/100)*page_height):\n",
    "                current_group.append(table)\n",
    "            else:\n",
    "                continued_tables.append(current_group)\n",
    "                current_group = [table]\n",
    "\n",
    "        if current_group:\n",
    "            continued_tables.append(current_group)\n",
    "        return continued_tables\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Table continuation detection failed: {e}\")\n",
    "\n",
    "def extract_with_camelot(pdf_name: str, pages: List[int]) -> List[Dict]:\n",
    "    \"\"\"Extract tables using Camelot with auto-merge\"\"\"\n",
    "    try:\n",
    "        tables = cam.read_pdf(\n",
    "            pdf_name,\n",
    "            pages=\",\".join(str(p+1) for p in pages),\n",
    "            flavor=\"lattice\",\n",
    "            suppress_stdout=True,\n",
    "            layout_kwargs={'detect_vertical': False}\n",
    "        )\n",
    "\n",
    "        if hasattr(cam, 'cleanup'):\n",
    "            cam.cleanup()\n",
    "\n",
    "        if not tables:\n",
    "            return []\n",
    "\n",
    "        grouped_tables = get_continued_tables(tables)\n",
    "        results = []\n",
    "\n",
    "        for group in grouped_tables:\n",
    "            headers = group[0].df.iloc[0].tolist()\n",
    "            rows = []\n",
    "            for table in group:\n",
    "                rows.extend([dict(zip(headers, row)) for row in table.df.iloc[1:].values])\n",
    "            results.append({\n",
    "                \"source\": \"camelot\",\n",
    "                \"headers\": headers,\n",
    "                \"rows\": rows,\n",
    "                \"page_range\": f\"{group[0].page}-{group[-1].page}\"\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Camelot extraction failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_with_pdfplumber(pdf_name: str, pages: List[int]) -> List[Dict]:\n",
    "    \"\"\"Fallback extraction with PDFPlumber\"\"\"\n",
    "    try:\n",
    "        results = []\n",
    "        with pdfplumber.open(pdf_name) as pdf:\n",
    "            for p in pages:\n",
    "                page = pdf.pages[p]\n",
    "                table = page.extract_table()\n",
    "                if table:\n",
    "                    headers = table[0]\n",
    "                    rows = [dict(zip(headers, row)) for row in table[1:]]\n",
    "                    results.append({\n",
    "                        \"source\": \"pdfplumber\",\n",
    "                        \"headers\": headers,\n",
    "                        \"rows\": rows,\n",
    "                        \"page_range\": str(p+1)\n",
    "                    })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"PDFPlumber extraction failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_pdf(pdf_name: str, keywords: List[str], output_json: str):\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    result = {\n",
    "        \"metadata\": {},\n",
    "        \"tables\": [],\n",
    "        \"warnings\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Classify pages\n",
    "        classification = classify_pdf_pages(pdf_name)\n",
    "        result[\"metadata\"][\"digital_pages\"] = create_page_ranges(classification[\"digital\"])\n",
    "        result[\"metadata\"][\"scanned_pages\"] = create_page_ranges(classification[\"scanned\"])\n",
    "\n",
    "        # Exit if no digital pages\n",
    "        if not classification[\"digital\"]:\n",
    "            result[\"warnings\"].append(\"PDF appears to be fully scanned\")\n",
    "            with open(output_json, \"w\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            return\n",
    "\n",
    "        # Locate table titles\n",
    "        title_pages = locate_table_title(pdf_name, keywords)\n",
    "        if not title_pages:\n",
    "            result[\"warnings\"].append(\"No matching table titles found\")\n",
    "            with open(output_json, \"w\") as f:\n",
    "                json.dump(result, f, indent=2)\n",
    "            return\n",
    "\n",
    "        # Process each found table\n",
    "        for title_page in title_pages:\n",
    "            table_pages = extract_table_pages(pdf_name, title_page)\n",
    "            if not table_pages:\n",
    "                result[\"warnings\"].append(f\"Found title but no tables on page {title_page+1}\")\n",
    "                continue\n",
    "\n",
    "            # Try Camelot first\n",
    "            tables = extract_with_camelot(pdf_name, table_pages)\n",
    "            if not tables:\n",
    "                # Fallback to PDFPlumber\n",
    "                tables = extract_with_pdfplumber(pdf_name, table_pages)\n",
    "                if not tables:\n",
    "                    result[\"warnings\"].append(f\"Failed to extract tables from pages {create_page_ranges(table_pages)}\")\n",
    "                    continue\n",
    "\n",
    "            result[\"tables\"].extend(tables)\n",
    "\n",
    "        # Save results\n",
    "        with open(output_json, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = str(e)\n",
    "        with open(output_json, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\n",
    "    \"Approved Makes and Manufacturer\",\n",
    "    \"Approved Manufacturers\",\n",
    "    \"List of Approved\",\n",
    "    \"List of Approved Makes\",\n",
    "    \"List of Manufacturers\",\n",
    "]\n",
    "\n",
    "#Enter PDF NAME and output file name\n",
    "process_pdf(\"pdf5.pdf\", keywords, \"hello.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
